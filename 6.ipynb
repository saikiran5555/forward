{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b68bad02",
   "metadata": {},
   "source": [
    "Backward propagation, also known as backpropagation, is a crucial step in training neural networks. Its primary purpose is to adjust the weights of the network based on the error between the predicted output and the actual output. Here's why backward propagation is important:\n",
    "\n",
    "Error Calculation: Backward propagation calculates the error between the predicted output of the network and the actual target output. This error quantifies how far off the network's prediction is from the desired outcome.\n",
    "\n",
    "Error Propagation: Backpropagation then propagates this error backward through the network, layer by layer, starting from the output layer and moving towards the input layer. It does this by applying the chain rule of calculus to compute the gradient of the error with respect to each weight and bias in the network.\n",
    "\n",
    "Weight Adjustment: Using the gradients calculated during backpropagation, the weights and biases of the network are adjusted in the opposite direction of the gradient. This adjustment is performed to minimize the error in subsequent iterations of training.\n",
    "\n",
    "Learning: By iteratively adjusting the weights and biases based on the errors calculated during backpropagation, the neural network learns to improve its predictions. The goal is to minimize the error between the predicted output and the actual output on the training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
