{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93c41163",
   "metadata": {},
   "source": [
    "Activation functions play a crucial role in neural networks during the forward propagation phase. They are used to introduce non-linear properties to the network, allowing it to learn complex patterns and relationships in the data. Without activation functions, a neural network would essentially be a linear regression model, incapable of handling the complexities found in real-world data such as images, videos, text, and sound.\n",
    "\n",
    "Here's how activation functions are used during forward propagation:\n",
    "\n",
    "Layer-wise Operation: In a neural network, each neuron in a layer receives input from the neurons in the previous layer. These inputs are weighted sums that include a bias term. After computing this weighted sum, the activation function is applied to the result.\n",
    "\n",
    "Introducing Non-linearity: The activation function processes the weighted sum, adding non-linear characteristics to the output. This non-linearity is what enables neural networks to approximate complex functions and solve problems that are not linearly separable.\n",
    "\n",
    "Forward Pass: During forward propagation, the process of applying weights, adding biases, and then applying an activation function is repeated for each layer in the network. The output of each layer becomes the input for the next layer until the final output layer is reached. The choice of activation function can affect the output range (e.g., whether it's bound between 0 and 1, or it can be any real number) and the learning dynamics of the network.\n",
    "\n",
    "Different Functions for Different Layers: Different types of activation functions are used in different parts of the network. For example, ReLU (Rectified Linear Unit) is commonly used in hidden layers due to its ability to reduce the vanishing gradient problem and speed up training. For binary classification tasks, a sigmoid function might be used in the output layer to squeeze the output between 0 and 1, representing a probability. For multi-class classification, a softmax function is often applied in the output layer to obtain probabilities that sum up to 1 across all classes.\n",
    "\n",
    "Backpropagation Relevance: Although activation functions are applied during forward propagation, their effects are also crucial during backpropagation, the phase where the network learns. The gradients of the activation functions are computed with respect to the loss function and are used to update the weights and biases in the network, optimizing the network's performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
