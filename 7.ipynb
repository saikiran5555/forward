{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27b88f80",
   "metadata": {},
   "source": [
    "Backpropagation, also known as backward propagation of errors, is a method used to update the weights of a neural network during training. In a single-layer feedforward neural network, the process of backward propagation involves calculating the gradients of the loss function with respect to the weights of the network.\n",
    "\n",
    "Let's consider a simple single-layer feedforward neural network with one input layer, one hidden layer, and one output layer. Here's how backward propagation is mathematically calculated step by step:\n",
    "\n",
    "Forward Propagation:\n",
    "\n",
    "Input data \n",
    "�\n",
    "x is passed through the network.\n",
    "The weighted sum of inputs \n",
    "�\n",
    "z is computed in the hidden layer using the following equation:\n",
    "�\n",
    "=\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "z=∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " w \n",
    "i\n",
    "​\n",
    " x \n",
    "i\n",
    "​\n",
    " +b\n",
    "The output of the hidden layer \n",
    "�\n",
    "a is obtained by applying an activation function \n",
    "�\n",
    "f to the weighted sum \n",
    "�\n",
    "z:\n",
    "�\n",
    "=\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "a=f(z)\n",
    "The output of the network \n",
    "�\n",
    "^\n",
    "y\n",
    "^\n",
    "​\n",
    "  is computed in the output layer using a similar process.\n",
    "Calculate Loss:\n",
    "\n",
    "Compute the loss between the predicted output \n",
    "�\n",
    "^\n",
    "y\n",
    "^\n",
    "​\n",
    "  and the actual output \n",
    "�\n",
    "y using a loss function \n",
    "�\n",
    "L.\n",
    "Backward Propagation:\n",
    "\n",
    "Compute the gradient of the loss function \n",
    "�\n",
    "L with respect to the output of the network:\n",
    "∂\n",
    "�\n",
    "∂\n",
    "�\n",
    "^\n",
    "∂ \n",
    "y\n",
    "^\n",
    "​\n",
    " \n",
    "∂L\n",
    "​\n",
    " \n",
    "Compute the gradient of the activation function \n",
    "�\n",
    "f with respect to the weighted sum \n",
    "�\n",
    "z in the hidden layer:\n",
    "∂\n",
    "�\n",
    "∂\n",
    "�\n",
    "∂z\n",
    "∂f\n",
    "​\n",
    " \n",
    "Use the chain rule to compute the gradient of the loss function with respect to the weighted sum \n",
    "�\n",
    "z in the hidden layer:\n",
    "∂\n",
    "�\n",
    "∂\n",
    "�\n",
    "=\n",
    "∂\n",
    "�\n",
    "∂\n",
    "�\n",
    "^\n",
    "×\n",
    "∂\n",
    "�\n",
    "^\n",
    "∂\n",
    "�\n",
    "∂z\n",
    "∂L\n",
    "​\n",
    " = \n",
    "∂ \n",
    "y\n",
    "^\n",
    "​\n",
    " \n",
    "∂L\n",
    "​\n",
    " × \n",
    "∂z\n",
    "∂ \n",
    "y\n",
    "^\n",
    "​\n",
    " \n",
    "​\n",
    " \n",
    "Compute the gradients of the loss function with respect to the weights \n",
    "�\n",
    "w and bias \n",
    "�\n",
    "b in the hidden layer:\n",
    "∂\n",
    "�\n",
    "∂\n",
    "�\n",
    "�\n",
    "=\n",
    "∂\n",
    "�\n",
    "∂\n",
    "�\n",
    "×\n",
    "∂\n",
    "�\n",
    "∂\n",
    "�\n",
    "�\n",
    "∂w \n",
    "i\n",
    "​\n",
    " \n",
    "∂L\n",
    "​\n",
    " = \n",
    "∂z\n",
    "∂L\n",
    "​\n",
    " × \n",
    "∂w \n",
    "i\n",
    "​\n",
    " \n",
    "∂z\n",
    "​\n",
    " \n",
    "∂\n",
    "�\n",
    "∂\n",
    "�\n",
    "=\n",
    "∂\n",
    "�\n",
    "∂\n",
    "�\n",
    "×\n",
    "∂\n",
    "�\n",
    "∂\n",
    "�\n",
    "∂b\n",
    "∂L\n",
    "​\n",
    " = \n",
    "∂z\n",
    "∂L\n",
    "​\n",
    " × \n",
    "∂b\n",
    "∂z\n",
    "​\n",
    " \n",
    "Update Weights:\n",
    "\n",
    "Use the gradients computed in the previous step to update the weights and biases of the network using an optimization algorithm such as gradient descent:\n",
    "�\n",
    "�\n",
    "←\n",
    "�\n",
    "�\n",
    "−\n",
    "�\n",
    "×\n",
    "∂\n",
    "�\n",
    "∂\n",
    "�\n",
    "�\n",
    "w \n",
    "i\n",
    "​\n",
    " ←w \n",
    "i\n",
    "​\n",
    " −α× \n",
    "∂w \n",
    "i\n",
    "​\n",
    " \n",
    "∂L\n",
    "​\n",
    " \n",
    "�\n",
    "←\n",
    "�\n",
    "−\n",
    "�\n",
    "×\n",
    "∂\n",
    "�\n",
    "∂\n",
    "�\n",
    "b←b−α× \n",
    "∂b\n",
    "∂L\n",
    "​\n",
    " \n",
    "where \n",
    "�\n",
    "α is the learning rate.\n",
    "Repeat:\n",
    "\n",
    "Repeat the process for a specified number of iterations or until convergence.\n",
    "This process is iteratively applied to train the neural network, adjusting the weights and biases to minimize the loss function and improve the network's performance on the training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
